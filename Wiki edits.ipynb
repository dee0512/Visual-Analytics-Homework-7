{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compsci 690V Homework 6\n",
    "This homework is done by **Devdhar Patel**.\n",
    "\n",
    "We pick the mini challenge 1 of VAST Challenge 2008 for this homework. \n",
    "\n",
    "**Description:** The Paraiso movement is controversial and is having considerable social impact in a specific area of the world. We have extracted a segment of the Paraiso (the movement) Wikipedia edits page. Please note this is not the Paraiso Manifesto Wiki page which is part of the background materials, but a related different page. Please use visual analytics to describe the social relationships of the editors (those that have edited/modified the Wikipedia page) as they are reflected in these files.\n",
    "\n",
    "**Note**: Some parts of this homework are from Homework 5. The homework 6 subsections are clearly marked.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a dataset similar to the one given in the mini challenge, we scraped the edits of the wikipedia page **\"2016â€“17 Kashmir unrest\"** (link: https://en.wikipedia.org/w/index.php?title=2016%E2%80%9317_Kashmir_unrest&limit=500&action=history). We chose this dataset since it is a recent revolutionary movement simirarly to the one described in the challenge. We scraped the text using the script in the file **getEdits.py**.\n",
    "\n",
    "We parsed the text of each commit into a dataframes with the columns:\n",
    "* **timestamp** - the timestamp of the edit\n",
    "* **user** - the user or ip of the editor\n",
    "* **minorEdit** - True or False - if the edit is minor\n",
    "* **pageLength** - Length of the page after the edit (in Bytes)\n",
    "* **editDiff** - bytes changed by the edit\n",
    "* **comment** - comment or description of the edit\n",
    "* **tags**\n",
    "* **entireEdit** - entire raw edit text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show,output_file,install_notebook_hook\n",
    "from bokeh.models import ColumnDataSource, Select,LabelSet, HoverTool, DatetimeAxis, TapTool, CustomJS, BoxZoomTool, PanTool\n",
    "from bokeh.models import WheelZoomTool, UndoTool, RedoTool, ResetTool, ZoomInTool, ZoomOutTool\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans,DBSCAN\n",
    "import pprint\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from bokeh.models.widgets import DataTable, TableColumn, Div, RangeSlider, Slider, Select\n",
    "from bokeh.layouts import Row,widgetbox, Column\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from bokeh.application.handlers import FunctionHandler\n",
    "from bokeh.application import Application\n",
    "from bokeh.palettes import Category10\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1338, 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "file = open('data/wiki sample data.txt','r', errors='ignore')\n",
    "\n",
    "lines = file.readlines()\n",
    "#print (lines)\n",
    "\n",
    "df = pd.DataFrame(columns=['timestamp','user','minorEdit','pageLength','editDiff','comment','tags','entireEdit'])\n",
    "i=0\n",
    "for line in lines:\n",
    "        entireEdit = line\n",
    "        #split via brackets\n",
    "        tokens = re.split('\\(|\\)',line)\n",
    "        timestamp = tokens[2].split('?')[0]\n",
    "        timestamp = pd.to_datetime(timestamp)\n",
    "        #split by ?\n",
    "        user = tokens[2].split('?')[1:]\n",
    "        user = ''.join(user)\n",
    "        user = user[1:-1]\n",
    "        m = tokens[4]\n",
    "        m= m.split('.')[0]\n",
    "        m= True if (m=='? m ') else False\n",
    "        byte = tokens[5].split(' ')[0]\n",
    "        byte = int(byte.replace(',',''))\n",
    "        change = tokens[7]\n",
    "        change = int(change.replace(',',''))\n",
    "        comment = re.split('\\. \\. ',line)[-1]\n",
    "        tagComment = comment.split('(undo)')\n",
    "        comment = tagComment[0][1:-2]\n",
    "        #undo is not at the end\n",
    "        if (len(tagComment)>1):\n",
    "            tag = tagComment[1][2:-2]\n",
    "            tag = tag.split(':')\n",
    "            if (len(tag)>1):\n",
    "                tag = tag[1][1:]\n",
    "                tag = tag.split(',')\n",
    "            if (tag[0]==''):\n",
    "                tag =[]\n",
    "        else:\n",
    "            tag=''\n",
    "\n",
    "        df.loc[i]=[timestamp,user,m,byte,change,comment,tag,entireEdit]\n",
    "        i+=1\n",
    "\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at the users who have made the most edits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DinoBambinoNFS     437\n",
      "Kautilya3          101\n",
      "Support2016         56\n",
      "117.214.245.178     29\n",
      "Thnidu              26\n",
      "Name: user, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "contributions = df['user'].value_counts()\n",
    "print(contributions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the edits have also been done by wikipedia bots. These bots usally have **'Bot'** at the end of their names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusikBot 2\n",
      "Bender the Bot 2\n",
      "ClueBot NG 2\n",
      "KolbertBot 1\n"
     ]
    }
   ],
   "source": [
    "for name in contributions.index.tolist():\n",
    "    if('Bot' in name):\n",
    "        print(name, contributions[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the edits that we just reverts of previous commits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edits that are reverts:  83\n"
     ]
    }
   ],
   "source": [
    "revertEdits = []\n",
    "for row in df.iterrows():\n",
    "    if('Revert' in row[1]['comment'] or 'revert' in row[1]['comment']):\n",
    "        revertEdits.append(row)\n",
    "\n",
    "print('Number of edits that are reverts: ',len(revertEdits))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Term Frequency-Inverse document Frequency feature extraction and clustering\n",
    "We will use TF-idf feature extraction and cluster the users using Kmeans clustering to see if there are any distinct clusters forming. To do that, we first create a dataframe **userWiseComments** to store all the comments from all the edits of a user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getUserWiseComments(dataframe):\n",
    "    userWiseComments = pd.DataFrame(columns=['user','allComments'])\n",
    "    users = list(set(dataframe['user']))\n",
    "    i = 0\n",
    "    for user in users:\n",
    "        userRows = df.loc[df['user'] == user]\n",
    "        allComments = ''\n",
    "        for row in userRows.iterrows():\n",
    "            allComments += row[1]['comment']\n",
    "        userWiseComments.loc[i] = [user,allComments]\n",
    "        i += 1\n",
    "    return userWiseComments\n",
    "userWiseComments = getUserWiseComments(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a tfidf matrix using the TfidfVectorizer and creating english stems of tokens for all comments of each user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "#take comments from dataframe\n",
    "commentsDf = userWiseComments['allComments']\n",
    "\n",
    "#token given text and find their stems\n",
    "def tokenizeAndStem(text):\n",
    "    CommentsTokens=[]\n",
    "    #for userComments in commentsDf:\n",
    "    CommentsTokens = (nltk.word_tokenize(text))\n",
    "    #filter out punctuations and numeric tokens\n",
    "    filtered_tokens = []\n",
    "    for token in CommentsTokens:\n",
    "        if re.search('[a-zA-Z0-9]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are some similarities in the features in each cluster. We try to plot it using dimensionality reduction via manifold reduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 6 Visualization 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"d49c2379-5888-4aec-965c-1450e2cb2ad4\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<script\n",
       "    src=\"http://localhost:63281/autoload.js?bokeh-autoload-element=5604576c-4efb-41d4-86e5-3a8aebf94c5c&bokeh-absolute-url=http://localhost:63281\"\n",
       "    id=\"5604576c-4efb-41d4-86e5-3a8aebf94c5c\"\n",
       "    data-bokeh-model-id=\"\"\n",
       "    data-bokeh-doc-id=\"\"\n",
       "></script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "530473767cf54b28acc3c062d5a6ea20"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_colors(clusters):\n",
    "    colors=[]\n",
    "    for i in clusters:\n",
    "        colors.append(Category10[10][i])\n",
    "        \n",
    "    return colors\n",
    "\n",
    "\n",
    "\n",
    "def modify_doc(doc):\n",
    "    def update(attr, old, new):\n",
    "        layout.children[2] = create_figure()\n",
    "    \n",
    "        \n",
    "    range_slider = RangeSlider(start=1,end=10, value=(1,4), step=1, title=\"N-grams Range\")\n",
    "    range_slider.on_change('value', update)\n",
    "    cluster_slider = Slider(start=1,end=10, value=3, step=1, title=\"Clusters\")\n",
    "    cluster_slider.on_change('value', update)\n",
    "    def create_figure():\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_df=1,\n",
    "                                 min_df=0, stop_words='english',\n",
    "                                 use_idf=False,tokenizer=tokenizeAndStem, ngram_range=(int(range_slider.value[0]),int(range_slider.value[1])))  #creating features by taking 3 words at a time\n",
    "\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(commentsDf)\n",
    "        num_clusters = cluster_slider.value\n",
    "        km = KMeans(n_clusters=num_clusters)\n",
    "        km.fit(tfidf_matrix)\n",
    "\n",
    "        clusters = km.labels_.tolist()\n",
    "        colors = get_colors(clusters)\n",
    "        mds = MDS(n_components=2, random_state=1)\n",
    "        pos = mds.fit_transform(tfidf_matrix.toarray())  # shape (n_components, n_samples)\n",
    "        xs, ys = pos[:, 0], pos[:, 1]\n",
    "        source = ColumnDataSource(data=dict(x=xs,y=ys,colors = colors, user = list(userWiseComments['user']), \n",
    "                                   comment = userWiseComments['allComments']))\n",
    "        hover = HoverTool(tooltips=[\n",
    "            (\"user\",'@user'),\n",
    "            (\"comment\",\"@comment\")\n",
    "        ])\n",
    "        b=figure( plot_height=500, plot_width=900, title='clustering groups based on edits', tools = [hover])\n",
    "        b.circle('x','y',fill_color='colors',line_color='colors',source=source,size=10)\n",
    "        return b\n",
    "    \n",
    "    p = create_figure()\n",
    "    layout = Column(range_slider,cluster_slider,p)\n",
    "    doc.add_root(layout)\n",
    "\n",
    "output_notebook()    \n",
    "handler = FunctionHandler(modify_doc)\n",
    "app = Application(handler)\n",
    "show(app,notebook_url='localhost:8888')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result:\n",
    "As we can see that this method is not very successful in grouping users and there are no clear clusters. This method would work better for actual text data and not comments in edits. However, we can see that on decreasing the range of n-grams, we the graph spread reduces. Therefore, increasing the range of n-grams analyzed might help. Unfortunately, we are working with very limited number of words in each comment and therefore it is not possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Method 2: Creating groups via reverts and mentions\n",
    "\n",
    "In this method we hypothesize that the users will be split into 4 major group:\n",
    "1. Bots.\n",
    "2. Neutral users (unbiased).\n",
    "3. Users aligned with Government of India, Indian Army and Central Reserve Police.\n",
    "4. Users aligned with Kashmiri protesters and separatists.\n",
    "\n",
    "Wikipedia bots have Bot at the end of their name, therefore getting group one is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yobot',\n",
      " 'GreenC bot',\n",
      " 'MusikBot',\n",
      " 'Bender the Bot',\n",
      " 'BG19bot',\n",
      " 'ClueBot NG',\n",
      " 'KolbertBot']\n"
     ]
    }
   ],
   "source": [
    "group1 = []\n",
    "group2 = []\n",
    "group3 = []\n",
    "group4 = []\n",
    "data = df\n",
    "for name in contributions.index.tolist():\n",
    "    if('bot' in name or 'Bot' in name):\n",
    "        group1.append(name)\n",
    "        data = data.loc[df['user']!= name]\n",
    "    \n",
    "pprint.pprint(group1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For group 3 and group 4 we analyze the reverts and mentions of usernames in the comments and try to find the opposing members from that. We extract phrases in which the usernames have been mentioned: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undid revision 730509214 by Kautilya3 \n",
      "Undid revision 742684927 by Fisaannath22 \n",
      "beforeReverted \u001b[43;3mto\u001b[m revision 741986450 by Kautilya3 \n",
      "alternative nameUndid revision 741101224 by Enaam1232 \n",
      "Reverted 1 pending edit by Jammukashmirwiki \n",
      "Kautilya3Reverted 1 pending edit by Ernasir \n",
      "Reverted 1 pending edit by 27.124.20.230 \n",
      "titleReverted 1 pending edit by 27.124.20.230 \n",
      "for any figuresReverted edits by 103.227.54.88 \n",
      "(talk): WP:NOTNEWS. (TW)Reverted edits by 39.46.245.15 \n",
      "Undid revision 801411509 by Mfarazbaig \n",
      "Tyler DurdenUndid revision 751079878 by Dscdscdsc11321 \n",
      "WP:NPOV??May 2017Reverted 1 edit by Amirk94391 \n",
      "in interimUndid revision 738200822 by Mar4d \n",
      "WP:Terrorist. (TW)Undid revision 730644572 by 1.186.211.178 \n",
      "guns: expandReverted 2 edits by 107.77.169.3 \n",
      "International angle.Undid revision 733204001 by DinoBambinoNFS \n",
      "Reverted \u001b[43;3mto\u001b[m revision 760253021 by Kautilya3 \n",
      "3dsxl \n",
      "thousands.Reverted 2 pending edits by StrivingAbdullah \n",
      "Reverted 2 pending edits by 114.143.207.162 \n",
      "infoReverted 1 pending edit by 123.136.107.90 \n",
      "talk page.Undid revision 732134892 by Durgahprasad \n",
      "two statements.Undid revision 732131295 by Durgahprasad \n",
      "single protest.Undid revision 732130687 by Durgahprasad \n",
      "sidesReverted 1 pending edit by 123.136.106.185 \n",
      "Undid revision 756817641 by Bojo1498 \n",
      "Reverted 1 edit by Sameehanagha \n",
      "(TW)Reverted good faith edits by Rayn123 \n",
      "WP:TERRORIST. (TW)Undid revision 730507557 by 45.121.116.10 \n",
      "see WP:SOFIXITUndid revision 730505728 by 45.121.116.10 \n",
      "the funeralReverted 2 edits by 210.212.203.66 \n",
      "WP:OR. (TW)Reverted 1 edit by Himbi133 \n",
      "the contentReverted 1 edit by 73.52.53.182 \n",
      "content. (TW)Undid revision 733056750 by 142.105.159.60 \n",
      "Kashmir unrestUndid revision 739994957 by Mkg20 \n",
      "(TW)Copyedit (minor)Reverted 1 edit by 169.149.184.134 \n",
      "using HotCatUndid revision 751179188 by Caknuck \n",
      "not propagandaReverted 7 edits by 120.59.190.141 \n",
      "discuss thereUndid revision 774237467 by Sameehanagha \n",
      "the sameReverted 7 edits by Sameehanagha \n",
      "edits. (TW)Undid revision 742742725 by Fisaannath22 \n",
      "emphasis. (TW)Reverted 1 edit by JagPantherSS \n",
      "the termReverted 10 edits by Sasyma \n",
      "(TW)Reverted \u001b[43;3mto\u001b[m revision 738492092 by DinoBambinoNFS \n",
      "(TW)Reverted \u001b[43;3mto\u001b[m revision 738189973 by DinoBambinoNFS \n",
      "edit (minor)Reverted 1 edit by 59.177.98.72 \n",
      "WP:NOTNEWS. (TW)Undid revision 756818081 by Irondome \n",
      "edits. (TW)Reverted 1 edit by 173.29.119.145 \n",
      "(TW)Reverted \u001b[43;3mto\u001b[m revision 742758265 by Kautilya3 \n",
      "reference(s)Reverted \u001b[43;3mto\u001b[m revision 738339167 by Mar4d \n",
      "claims. (TW)Reverted 1 edit by 96.90.200.133 \n",
      "an entryReverted 3 edits by Kushagr.sharma1 \n",
      "talk pageReverted 3 edits by 141.241.26.20 \n",
      "talk pageUndid revision 732211469 by Saqib \n",
      "ValleyExpanding articleReverted 1 edit by 103.249.38.248 \n",
      "factual accuracyReverted 3 edits by WERWER32423424 \n",
      "with sourcesReverted 1 edit by MalikAttaRasool \n",
      "reliable sourcesReverted 3 edits by 49.201.2.96 \n",
      "military confrontationUndid revision 751079238 by Dscdscdsc11321 \n",
      "page. (TW)Reverted 1 edit by Thnidu \n",
      "Expanding articleReverted 2 edits by 125.21.241.186 \n",
      "covered aboveReverted 1 edit by Bishupriyaparam \n",
      "edits. (TW)Reverted 1 edit by 74.142.132.230 \n",
      "consensus. (TW)Reverted 3 edits by Iammomin \n",
      "with attributions....Undid revision 775020709 by Sameehanagha \n",
      "talk page.Undid revision 773241364 by Zulkarnainbanday \n",
      "the titleUndid revision 773241437 by Zulkarnainbanday \n",
      "Undid revision 762266465 by Mfarazbaig \n",
      "Reverted 1 pending edit by Asarqwsewerwer \n",
      "Reverted edits by 213.78.73.72 \n",
      "Indian usersUndid revision 729920453 by Kautilya3 \n",
      "inadequateReverted 4 pending edits by AasifNP \n",
      "pending edits by AasifNP and 49.201.42.169 \n",
      "Reverted 1 pending edit by 43.224.128.168 \n",
      "WikipediaReverted 6 pending edits by Umerrashid1933 \n",
      "before savingReverted 1 edit by 110.224.202.142 \n",
      "wrongReverted 1 pending edit by 47.9.204.176 \n",
      "Undid revision 730239226 by Support2016 \n",
      "Reverted 1 pending edit by 180.179.74.201 \n",
      "Undid revision 732135483 by DinoBambinoNFS \n",
      "Undid revision 730251908 by 122.178.197.15 \n",
      "Wani's deathUndid revision 729869237 by Spartacus! \n",
      "Reverted edits by 58.173.33.229 \n",
      "(talk) \u001b[43;3mto\u001b[m last version by Richi \n",
      "(talk) \u001b[43;3mto\u001b[m last version by Laxmanlappy \n",
      "Reverted edits by 106.219.8.153 \n",
      "Reverted good faith edits by Kikobiko10 \n",
      "Reverted 1 pending edit by Zouhaib \n",
      "Barthateslisa \n",
      "Reverted edits by Kashmirkikali06 \n",
      "Undid revision 733200488 by 117.203.199.85 \n",
      "(TW)Reverted \u001b[43;3mto\u001b[m revision 730013591 by 43.247.158.5 \n",
      "edits.Reverted \u001b[43;3mto\u001b[m revision 733096398 by Kautilya3 \n"
     ]
    }
   ],
   "source": [
    "userWiseComments2 = getUserWiseComments(data)\n",
    "userMentions = pd.DataFrame(columns=['user','mention','phrase', 'to'])\n",
    "listOfUserNames = list(userWiseComments2['user'])\n",
    "for row in userWiseComments2.iterrows():\n",
    "    tokens = row[1]['allComments'].split()\n",
    "    for name in listOfUserNames:\n",
    "        indexes = [i for i,token in enumerate(tokens) if token==name]\n",
    "        for i in indexes:\n",
    "            start = max(0, i-5)\n",
    "            phrase = \" \".join(tokens[start:i+1])\n",
    "            printText = \"\"\n",
    "            to = False\n",
    "            for word in tokens[start:i+1]:\n",
    "                if(word == 'to' or word == 'To'):\n",
    "                    printText +=\"\\033[43;3m\"+word+\"\\033[m \"\n",
    "                    to = True\n",
    "                else:\n",
    "                    printText +=word + \" \"\n",
    "            print(printText)\n",
    "            userMentions.loc[len(userMentions)] = [row[1]['user'],name,phrase,to]\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above phrases, we can see that when the word to is included in the phrase, the mention is done in a positive sense. Otherwise it is done in a negative sense. We will attempt to create a relationship matrix on the following criteria: \n",
    "1. if the mention is used in a positive way, we will add 1 to the relationship\n",
    "2. if the mention is used in a negative way, we will subtract 1 to the relationship\n",
    "3. All relationships start at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relationshipMatrix = {}\n",
    "for name1 in listOfUserNames:\n",
    "    relationshipMatrix[name1] = {}\n",
    "    for name2 in listOfUserNames:\n",
    "        relationshipMatrix[name1][name2] = 0\n",
    "        \n",
    "for row in userMentions.iterrows():\n",
    "    if(row[1]['to'] == True):\n",
    "        if(row[1]['user']<row[1]['mention']):\n",
    "            relationshipMatrix[row[1]['user']][row[1]['mention']] += 1\n",
    "        else:\n",
    "            relationshipMatrix[row[1]['mention']][row[1]['user']] += 1\n",
    "    else:\n",
    "        if(row[1]['user']<row[1]['mention']):\n",
    "            relationshipMatrix[row[1]['user']][row[1]['mention']] -= 1\n",
    "        else:\n",
    "            relationshipMatrix[row[1]['mention']][row[1]['user']] -= 1\n",
    "\n",
    "relationships = pd.DataFrame(columns=['user1','user2','relationship'])\n",
    "i=0\n",
    "for key1,dictionary in relationshipMatrix.items():\n",
    "        for key2,value in dictionary.items():\n",
    "            if(value != 0):\n",
    "                relationships.loc[i] = [key1,key2,value]\n",
    "                i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 6 Visualization 3:\n",
    "\n",
    "In this visualization, we explore the relationship matrix I built for the second method of grouping. The relationship matrix shows the relation of each user with other users. The more green a pixel is the more positive the relationship is and the more red a pixel is the more negative a relationship is. I have removed the users who do not have any kind of relationship with any other users to make the graph less empty. On clicking each of the pixels, we can view a summary of the relationship between the two users the pixel represents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"094b73ea-5770-4e04-8d20-9012568239ca\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {},
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "ca7fab1f-d034-4969-adaf-278d2e0d0965"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "green =['#AFF7A1','#5FF042','#23970C','#092603'] \n",
    "red = ['#FDBBB0','#F9674D','#DA2607','#8B1804']\n",
    "heatmap = pd.DataFrame(columns = ['x','y','value','color','phrases'])\n",
    "\n",
    "\n",
    "for key1,dictionary in relationshipMatrix.items():\n",
    "        for key2,value in dictionary.items():\n",
    "            if(value != 0):\n",
    "                if(value < 0):\n",
    "                    color = red[abs(value) - 1]\n",
    "                else:\n",
    "                    color = green[abs(value) - 1]\n",
    "                userRows = userMentions.loc[(userMentions['user'] == key1) & (userMentions['mention'] == key2)]\n",
    "                mentionRows = userMentions.loc[(userMentions['mention'] == key1) & (userMentions['user'] == key2)]\n",
    "                phrases = list(userRows['phrase']) + list(mentionRows['phrase'])\n",
    "                heatmap.loc[len(heatmap)] = [key1,key2,value,color,phrases]\n",
    "                heatmap.loc[len(heatmap)] = [key2,key1,value,color,phrases]\n",
    "             \n",
    "c=figure( plot_height=900, plot_width=900, title='Relationship Heatmap',y_range=list(set(heatmap['y'])),\n",
    "         x_range=list(set(heatmap['y'])), \n",
    "         tools = [TapTool(),BoxZoomTool(), PanTool(),WheelZoomTool(), UndoTool(), RedoTool(), ResetTool(), ZoomInTool(), ZoomOutTool()])\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x=heatmap['x'],\n",
    "    y=heatmap['y'],\n",
    "    color = heatmap['color'],\n",
    "    value = heatmap['value'],\n",
    "    phrases=  heatmap['phrases']\n",
    "))\n",
    "c.yaxis.major_label_text_font_size = \"6pt\"\n",
    "c.xaxis.major_label_text_font_size = \"6pt\"\n",
    "c.xaxis.major_label_orientation = math.pi/2\n",
    "c.square('x','y',color='color',source=source,size=7)\n",
    "\n",
    "code2 = \"\"\"\n",
    "    console.log('abc')\n",
    "    var data = source.data,\n",
    "    selected = source.selected['1d']['indices'],\n",
    "    select_inds = selected[0];\n",
    "    console.log('abc')\n",
    "    heading = \"<p>Relationship between <b>\"+data['x'][select_inds]+\"</b> and <b>\" +data['y'][select_inds]+ \"</b>:</p>\"\n",
    "    value = \"<p><b>Value: </b>\"+data['value'][select_inds]+\"</p>\"\n",
    "    phraseHeading = \"<p><b>Phrases:</b></p>\"\n",
    "    phrases = \"\"\n",
    "    for(var i = 0; i< data['phrases'][select_inds].length; i++){\n",
    "        phrases = phrases + '<p>' +data['phrases'][select_inds][i]+ '</p>'\n",
    "    }\n",
    "    div.text = heading+value+phraseHeading+ phrases\n",
    "    console.log(div.text)\n",
    "\"\"\"\n",
    "\n",
    "div2 = Div(text=\"\",\n",
    "width=400, height=100)\n",
    "source.callback = CustomJS(args={'source':source, 'div':div2},code=code2)\n",
    "show(Column(c,div2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** I believe that this method is the most accurate of all the three methods. The visualization allows us to explore the phrases on which the relationship is measured and the value of the relationship. This visualization provides a good insight on how we are calculating the relationships and on the intraction of each user with other users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create groups, we start by taking the most active users in the **relationships** dataframe and grouping according to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relationshipUsers = list(relationships['user1'])+list(relationships['user2'])\n",
    "uniqueUsers = set(relationshipUsers)\n",
    "userCount = {}\n",
    "for user in uniqueUsers:\n",
    "    userCount[user] = relationshipUsers.count(user)\n",
    "    \n",
    "userCount = pd.Series(userCount)\n",
    "userCount = userCount.sort_values(ascending=False)\n",
    "for user in userCount.index:\n",
    "    \n",
    "    if(user not in group3 and user not in group4):\n",
    "        group3.append(user)\n",
    "        \n",
    "    if(user in group3):\n",
    "        rows1 = relationships.loc[relationships['user1'] == user]\n",
    "        rows2 = relationships.loc[relationships['user2'] == user]\n",
    "        for row in rows1.iterrows():\n",
    "            user2 = row[1]['user2']\n",
    "            if(user2 in group3 or user2 in group4):\n",
    "                continue\n",
    "            elif(row[1]['relationship']>0):\n",
    "                group3.append(user2)\n",
    "            else:\n",
    "                group4.append(user2)\n",
    "                \n",
    "        for row in rows2.iterrows():\n",
    "            user2 = row[1]['user1']\n",
    "            if(user2 in group3 or user2 in group4):\n",
    "                continue\n",
    "            elif(row[1]['relationship']>0):\n",
    "                group3.append(user2)\n",
    "            else:\n",
    "                group4.append(user2)\n",
    "        \n",
    "    if(user in group4):\n",
    "        rows1 = relationships.loc[relationships['user1'] == user]\n",
    "        rows2 = relationships.loc[relationships['user2'] == user]\n",
    "        for row in rows1.iterrows():\n",
    "            user2 = row[1]['user2']\n",
    "            if(user2 in group3 or user2 in group4):\n",
    "                continue\n",
    "            elif(row[1]['relationship']>0):\n",
    "                group4.append(user2)\n",
    "            else:\n",
    "                group3.append(user2)\n",
    "                \n",
    "        for row in rows2.iterrows():\n",
    "            user2 = row[1]['user1']\n",
    "            if(user2 in group3 or user2 in group4):\n",
    "                continue\n",
    "            elif(row[1]['relationship']>0):\n",
    "                group4.append(user2)\n",
    "            else:\n",
    "                group3.append(user2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we put all the remaining users in **group 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for user in listOfUserNames:\n",
    "    if(user not in group3 and user not in group4):\n",
    "        group2.append(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The groups that we thus get are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"90bc358d-d1d0-41d0-9059-b460d9a7a5b6\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {},
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "8d0f858e-7184-430b-ab8e-9ee98ba2641e"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "group1Source = ColumnDataSource(dict(names = group1))\n",
    "group2Source = ColumnDataSource(dict(names = group2))\n",
    "group3Source = ColumnDataSource(dict(names = group3))\n",
    "group4Source = ColumnDataSource(dict(names = group4))\n",
    "\n",
    "\n",
    "data_table1 = DataTable(source=group1Source, columns=[TableColumn(field=\"names\", title=\"Group 1 (Bots)\")], width=180, height=480,\n",
    "                        reorderable = False, row_headers= False)\n",
    "data_table2 = DataTable(source=group2Source, columns=[TableColumn(field=\"names\", title=\"Group 2 (Neutral)\")], width=200, height=480,\n",
    "                        reorderable = False, row_headers= False)\n",
    "data_table3 = DataTable(source=group3Source, columns=[TableColumn(field=\"names\", title=\"Group 3\")], width=200, height=480,\n",
    "                        reorderable = False, row_headers= False)\n",
    "data_table4 = DataTable(source=group4Source, columns=[TableColumn(field=\"names\", title=\"Group 4\")], width=200, height=480,\n",
    "                        reorderable = False, row_headers= False)\n",
    "\n",
    "show(Row(widgetbox(data_table1, width=200),\n",
    "         widgetbox(data_table2, width=220),\n",
    "         widgetbox(data_table3, width=220),\n",
    "         widgetbox(data_table4, width=220)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result:\n",
    "This method is the most promising of all the methods as it is possible that the two opposite groups would revert and mention each other a lot. Thus this partition is very probable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Creating groups via sentiment analysis\n",
    "In this method we try to analysis the attitude of the users by the sentiment of their comments. We use the **VADER sentiment analysis tool**:\n",
    "\n",
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n",
    "\n",
    "Again we remove all the bots first and the group them by netural, positive and negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "green ='#23970C'\n",
    "red = '#DA2607'\n",
    "group1 = []\n",
    "group2 = []\n",
    "group3 = []\n",
    "group4 = []\n",
    "neutrals = pd.DataFrame(columns = ['user','allComments','compound', 'color'])\n",
    "positives = pd.DataFrame(columns = ['user','allComments','compound', 'color'])\n",
    "negatives = pd.DataFrame(columns = ['user','allComments','compound', 'color'])\n",
    "compound = {}\n",
    "data = df\n",
    "for name in contributions.index.tolist():\n",
    "    if('bot' in name or 'Bot' in name):\n",
    "        group1.append(name)\n",
    "        data = data.loc[df['user']!= name]\n",
    "        \n",
    "sia = SentimentIntensityAnalyzer()\n",
    "userWiseComments2 = getUserWiseComments(data)\n",
    "for row in userWiseComments2.iterrows():\n",
    "    allComments = row[1]['allComments']\n",
    "    res = sia.polarity_scores(allComments)\n",
    "    compound[allComments] = res['compound']\n",
    "    if res['compound'] > 0.2:\n",
    "        group3.append(row[1]['user'])\n",
    "        positives.loc[len(neutrals)] = [row[1]['user'],allComments,res['compound'],green]\n",
    "    elif res['compound'] < -0.2:\n",
    "        group4.append(row[1]['user'])\n",
    "        negatives.loc[len(neutrals)] = [row[1]['user'],allComments,res['compound'],red]\n",
    "    else:\n",
    "        group2.append(row[1]['user'])\n",
    "        if(res['compound'] < 0):\n",
    "            color = red\n",
    "        else:\n",
    "            color = green\n",
    "        neutrals.loc[len(neutrals)] = [row[1]['user'],allComments,res['compound'], color]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 6 Visualization 4:\n",
    "In this visualization, we explore the algorithm of sentiment analysis. We can look at 3 different graphs of each of the different groups of users: netutral, positive and negative. Each graph shows the index of the sentiment of each users. We categorize values above 0.2 as positive and below -0.2 as negative and other values as neutral. Upon selecting the group, we are also able to select individual users and look at the sentiment of their individual comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"08f12efb-2e0c-4f3c-b64c-62c2ce5712f2\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<script\n",
       "    src=\"http://localhost:63293/autoload.js?bokeh-autoload-element=637adeec-ccb4-4bc8-bc00-892b7d5df7e2&bokeh-absolute-url=http://localhost:63293\"\n",
       "    id=\"637adeec-ccb4-4bc8-bc00-892b7d5df7e2\"\n",
       "    data-bokeh-model-id=\"\"\n",
       "    data-bokeh-doc-id=\"\"\n",
       "></script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "c77afd078b074d63bef002ef10c86675"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def modify_doc1(doc):\n",
    "    def update(attr, old, new):\n",
    "        layout.children[1] = create_figure()\n",
    "        layout.children[0].children[1] = getUserSelect()\n",
    "    \n",
    "    sentimentSelect = Select(title=\"Select Group:\", value=\"Neutrals\", options=[\"Neutrals\", \"Positives\", \"Negatives\"])\n",
    "    sentimentSelect.on_change('value', update)\n",
    "    def create_figure():\n",
    "        if(sentimentSelect.value == 'Neutrals'):\n",
    "            users = list(neutrals['user'])\n",
    "            source = ColumnDataSource(data=dict(x=list(neutrals['user']),y=list(neutrals['compound']),\n",
    "                                                colors = list(neutrals['color'])))\n",
    "        elif(sentimentSelect.value == 'Positives'):\n",
    "            users = list(positives['user'])\n",
    "            source = ColumnDataSource(data=dict(x=list(positives['user']),y=list(positives['compound']),\n",
    "                                                colors = list(positives['color'])))\n",
    "        else:\n",
    "            users = list(negatives['user'])\n",
    "            source = ColumnDataSource(data=dict(x=list(negatives['user']),y=list(negatives['compound']),\n",
    "                                                colors = list(negatives['color'])))  \n",
    "            \n",
    "        \n",
    "        \n",
    "        d=figure( plot_height=500, plot_width=900, title='Sentiment Analysis',x_range=users)\n",
    "        d.vbar(x='x',top='y',bottom=0,fill_color='colors',width=0.5,source=source)\n",
    "        d.xaxis.major_label_text_font_size = \"6pt\"\n",
    "        d.xaxis.major_label_orientation = math.pi/2\n",
    "        return d\n",
    "    \n",
    "    def get_div(value):\n",
    "        if(sentimentSelect.value == 'Neutrals'):\n",
    "            data = neutrals\n",
    "        elif(sentimentSelect.value == 'Positives'):\n",
    "            data = positives\n",
    "        else:\n",
    "            data = negatives\n",
    "        user = value\n",
    "        userRow = data.loc[data['user'] == user]\n",
    "        text = \"<h3>\"+user+\"</h3>\"\n",
    "        text = text + \"<p><b>compound:</b> \" + str(userRow['compound'].values[0]) + \"</p>\" \n",
    "        text = text + '<table><tr><th>Comment</th><th>Compound</th></tr>'\n",
    "        comments = df.loc[df['user'] == user]\n",
    "        for row in comments.iterrows():\n",
    "            comment = row[1]['comment']\n",
    "            res = sia.polarity_scores(comment)\n",
    "            text = text + \"<tr><td>\"+ comment + \"</td><td>\"+str(res['compound']) + \"</td></tr>\"\n",
    "        text = text + \"</table>\"\n",
    "        return text\n",
    "    \n",
    "    def setDiv(attr, old, new):\n",
    "        div.text =  get_div(new)\n",
    "    \n",
    "    def getUserSelect():\n",
    "        if(sentimentSelect.value == 'Neutrals'):\n",
    "            users = list(neutrals['user'])\n",
    "        elif(sentimentSelect.value == 'Positives'):\n",
    "            users = list(positives['user'])\n",
    "        else:\n",
    "            users = list(negatives['user'])\n",
    "        userSelect = Select(title=\"Select User:\", value=users[0], options=users)\n",
    "        userSelect.on_change('value', setDiv)\n",
    "        div.text =  get_div(users[0])\n",
    "        return userSelect\n",
    "    \n",
    "    \n",
    "    p = create_figure()\n",
    "    div = Div(text=\"\",height=100,width=700)\n",
    "    userSelect = getUserSelect()\n",
    "    layout = Column(Row(sentimentSelect,userSelect),p,div)\n",
    "    doc.add_root(layout)\n",
    "\n",
    "output_notebook()    \n",
    "handler1 = FunctionHandler(modify_doc1)\n",
    "app1 = Application(handler1)\n",
    "show(app1,notebook_url='localhost:8888')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** This graph provides a good insight on what text the algoritm classifies as neutral, positive and negative. Upon exploring this visualization more, I observed that some of the text has not been correctly classified. For example the text: \"please do not BLINDLY REVERT everything. you r opposition was to a paragraph (that has not even be removed!)\" has been classified as positive with a compound value of 0.3802. However, the text is clearly negative. This suggests that we would need to tweak the algorithm a bit to run it on comments of wiki-edits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"da34a317-4ce6-43fc-b85d-695ebe80860f\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {},
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "9a84fb5f-d640-44aa-a85a-f0c040a24177"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "group1Source = ColumnDataSource(dict(names = group1))\n",
    "group2Source = ColumnDataSource(dict(names = group2))\n",
    "group3Source = ColumnDataSource(dict(names = group3))\n",
    "group4Source = ColumnDataSource(dict(names = group4))\n",
    "\n",
    "\n",
    "data_table1 = DataTable(source=group1Source, columns=[TableColumn(field=\"names\", title=\"Group 1 (Bots)\")], width=180, height=480,\n",
    "                        reorderable = False, row_headers= False)\n",
    "data_table2 = DataTable(source=group2Source, columns=[TableColumn(field=\"names\", title=\"Group 2 (Neutral)\")], width=200, height=480,\n",
    "                        reorderable = False, row_headers= False)\n",
    "data_table3 = DataTable(source=group3Source, columns=[TableColumn(field=\"names\", title=\"Group 3 (Positive)\")], width=200, height=480,\n",
    "                        reorderable = False, row_headers= False)\n",
    "data_table4 = DataTable(source=group4Source, columns=[TableColumn(field=\"names\", title=\"Group 4 (Negative)\")], width=200, height=480,\n",
    "                        reorderable = False, row_headers= False)\n",
    "\n",
    "show(Row(widgetbox(data_table1, width=200),\n",
    "         widgetbox(data_table2, width=220),\n",
    "         widgetbox(data_table3, width=220),\n",
    "         widgetbox(data_table4, width=220)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result:\n",
    "This method is successful in grouping users by their attitudes. However, this method alone would not be able to give the grouping of each faction. Combining this method with method 2 would help improve the result. Overall, method 2 is the most promising method for solving the Vast 2008 Mini challenge 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
